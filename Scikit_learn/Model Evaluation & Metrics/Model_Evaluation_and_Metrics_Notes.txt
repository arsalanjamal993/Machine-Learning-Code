📘 MACHINE LEARNING NOTES — MODEL EVALUATION & METRICS

──────────────────────────────────────────────
🎯 TOPIC: MODEL EVALUATION & METRICS
──────────────────────────────────────────────

1️⃣ DEFINITION:
Model Evaluation Metrics are methods to measure how good or bad your Machine Learning model performs.
They compare your model’s **predictions** with the **actual real outcomes**.

──────────────────────────────────────────────
2️⃣ WHY IT MATTERS:
Without evaluation, you don’t know if your model is guessing right or wrong.
Metrics give your model a **report card** — showing accuracy, precision, recall, or how big the errors are.

──────────────────────────────────────────────
3️⃣ MAIN TYPES OF METRICS:

=========================
⚙️ CLASSIFICATION METRICS
=========================

💡 Used when the model predicts categories (Yes/No, Spam/Not Spam, 1/0).

-------------------------------------------------------------
📍 ACCURACY
-------------------------------------------------------------
📖 Definition:
Percentage of total correct predictions made by the model.

🧠 Formula:
Accuracy = (Correct Predictions) / (Total Predictions)

🔍 Example:
Out of 100 emails, if 90 are correctly labeled → Accuracy = 90/100 = 0.9 or 90%

🧩 Syntax:
from sklearn.metrics import accuracy_score
accuracy_score(y_true, y_pred)

💻 Code Example:
from sklearn.metrics import accuracy_score
y_true = [1, 0, 1, 1, 0]
y_pred = [1, 0, 0, 1, 1]
print("Accuracy:", accuracy_score(y_true, y_pred))

⚙ How It Works:
- Compares prediction and actual values.
- Counts how many are the same.
- Divides by total samples.

-------------------------------------------------------------
📍 PRECISION
-------------------------------------------------------------
📖 Definition:
Out of all the positive predictions your model made, how many were actually correct.

🧠 Formula:
Precision = TP / (TP + FP)
(TP = True Positive, FP = False Positive)

🔍 Example:
If model predicted 10 patients have disease but only 8 actually do → Precision = 8/10 = 0.8

🧩 Syntax:
from sklearn.metrics import precision_score
precision_score(y_true, y_pred)

⚙ How It Works:
- Focuses on how accurate your “positive” predictions are.
- High precision means fewer false alarms.

-------------------------------------------------------------
📍 RECALL (Sensitivity)
-------------------------------------------------------------
📖 Definition:
Out of all the actual positive cases, how many did the model correctly identify.

🧠 Formula:
Recall = TP / (TP + FN)
(FN = False Negative)

🔍 Example:
Out of 10 sick patients, model caught 8 → Recall = 8/10 = 0.8

🧩 Syntax:
from sklearn.metrics import recall_score
recall_score(y_true, y_pred)

⚙ How It Works:
- Measures how many true positives your model successfully finds.
- High recall means fewer missed positive cases.

-------------------------------------------------------------
📍 F1 SCORE
-------------------------------------------------------------
📖 Definition:
A balance between Precision and Recall — good when classes are imbalanced.

🧠 Formula:
F1 = 2 * (Precision * Recall) / (Precision + Recall)

🔍 Example:
If Precision = 0.8 and Recall = 0.6 → F1 = 2 * (0.8*0.6)/(0.8+0.6) = 0.69

🧩 Syntax:
from sklearn.metrics import f1_score
f1_score(y_true, y_pred)

⚙ How It Works:
- Combines precision and recall.
- High F1 means model is good at both detecting positives and avoiding false alarms.

-------------------------------------------------------------
📍 CONFUSION MATRIX
-------------------------------------------------------------
📖 Definition:
A table showing counts of predictions vs actuals for all classes.

🧩 Syntax:
from sklearn.metrics import confusion_matrix
confusion_matrix(y_true, y_pred)

🔍 Example Output:
[[50 10]
 [5  35]]
→ 50 = True Negative, 10 = False Positive, 5 = False Negative, 35 = True Positive

⚙ How It Works:
- Rows = Actual values
- Columns = Predicted values
- Helps visualize model performance in detail.

=========================
⚙️ REGRESSION METRICS
=========================

💡 Used when model predicts continuous numbers (e.g., prices, scores, temperature).

-------------------------------------------------------------
📍 MAE (Mean Absolute Error)
-------------------------------------------------------------
📖 Definition:
Average of absolute (non-negative) differences between predicted and actual values.

🧠 Formula:
MAE = (Σ|y_true - y_pred|) / n

🔍 Example:
Actual: [10, 20, 30], Predicted: [12, 18, 33]
→ |10-12| + |20-18| + |30-33| = 2+2+3 = 7 → 7/3 = 2.33

🧩 Syntax:
from sklearn.metrics import mean_absolute_error

⚙ How It Works:
1. Take the difference between prediction and actual.
2. Remove the negative sign (absolute value).
3. Add all differences and divide by total samples.
→ Shows average “off by” amount.

-------------------------------------------------------------
📍 MSE (Mean Squared Error)
-------------------------------------------------------------
📖 Definition:
Average of squared differences between predicted and actual values.

🧠 Formula:
MSE = (Σ(y_true - y_pred)²) / n

🔍 Example:
Actual: [10, 20], Predicted: [12, 17]
→ (10-12)² + (20-17)² = 4 + 9 = 13 → 13/2 = 6.5

🧩 Syntax:
from sklearn.metrics import mean_squared_error

⚙ How It Works:
1. Take each error, square it.
2. Add them all up.
3. Divide by total samples.
→ Large errors become more noticeable because of squaring.

-------------------------------------------------------------
📍 RMSE (Root Mean Squared Error)
-------------------------------------------------------------
📖 Definition:
Square root of MSE — gives a more interpretable “real-world” average error.

🧠 Formula:
RMSE = √MSE

🔍 Example:
If MSE = 6.5 → RMSE = √6.5 = 2.55

🧩 Syntax:
import numpy as np
rmse = np.sqrt(mse)

⚙ How It Works:
- Same as MSE, but takes square root.
- Makes error units same as original data (e.g., dollars, marks, temperature).

──────────────────────────────────────────────
🧭 SUMMARY TABLE

| Metric | Type | Measures | Good When |
|--------|------|-----------|------------|
| Accuracy | Classification | Overall correctness | Balanced data |
| Precision | Classification | Correct positives | False alarms costly |
| Recall | Classification | Found positives | Missing positives costly |
| F1 Score | Classification | Balance of precision & recall | Imbalanced data |
| MAE | Regression | Avg. absolute error | Simple interpretation |
| MSE | Regression | Avg. squared error | Punish large errors |
| RMSE | Regression | Root of MSE | Real-world scale error |

──────────────────────────────────────────────
💡 FINAL CONCEPT MEMORY TRICK:
- **Accuracy** → “How often am I right?”  
- **Precision** → “When I say YES, am I correct?”  
- **Recall** → “Did I catch all the YES cases?”  
- **F1** → “Smart balance of precision & recall.”  
- **MAE** → “On average, how far off?”  
- **MSE** → “Big mistakes hit harder.”  
- **RMSE** → “Realistic error you can understand.”

──────────────────────────────────────────────

