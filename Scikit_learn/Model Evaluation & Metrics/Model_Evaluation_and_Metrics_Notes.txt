ğŸ“˜ MACHINE LEARNING NOTES â€” MODEL EVALUATION & METRICS

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ¯ TOPIC: MODEL EVALUATION & METRICS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1ï¸âƒ£ DEFINITION:
Model Evaluation Metrics are methods to measure how good or bad your Machine Learning model performs.
They compare your modelâ€™s **predictions** with the **actual real outcomes**.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2ï¸âƒ£ WHY IT MATTERS:
Without evaluation, you donâ€™t know if your model is guessing right or wrong.
Metrics give your model a **report card** â€” showing accuracy, precision, recall, or how big the errors are.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
3ï¸âƒ£ MAIN TYPES OF METRICS:

=========================
âš™ï¸ CLASSIFICATION METRICS
=========================

ğŸ’¡ Used when the model predicts categories (Yes/No, Spam/Not Spam, 1/0).

-------------------------------------------------------------
ğŸ“ ACCURACY
-------------------------------------------------------------
ğŸ“– Definition:
Percentage of total correct predictions made by the model.

ğŸ§  Formula:
Accuracy = (Correct Predictions) / (Total Predictions)

ğŸ” Example:
Out of 100 emails, if 90 are correctly labeled â†’ Accuracy = 90/100 = 0.9 or 90%

ğŸ§© Syntax:
from sklearn.metrics import accuracy_score
accuracy_score(y_true, y_pred)

ğŸ’» Code Example:
from sklearn.metrics import accuracy_score
y_true = [1, 0, 1, 1, 0]
y_pred = [1, 0, 0, 1, 1]
print("Accuracy:", accuracy_score(y_true, y_pred))

âš™ How It Works:
- Compares prediction and actual values.
- Counts how many are the same.
- Divides by total samples.

-------------------------------------------------------------
ğŸ“ PRECISION
-------------------------------------------------------------
ğŸ“– Definition:
Out of all the positive predictions your model made, how many were actually correct.

ğŸ§  Formula:
Precision = TP / (TP + FP)
(TP = True Positive, FP = False Positive)

ğŸ” Example:
If model predicted 10 patients have disease but only 8 actually do â†’ Precision = 8/10 = 0.8

ğŸ§© Syntax:
from sklearn.metrics import precision_score
precision_score(y_true, y_pred)

âš™ How It Works:
- Focuses on how accurate your â€œpositiveâ€ predictions are.
- High precision means fewer false alarms.

-------------------------------------------------------------
ğŸ“ RECALL (Sensitivity)
-------------------------------------------------------------
ğŸ“– Definition:
Out of all the actual positive cases, how many did the model correctly identify.

ğŸ§  Formula:
Recall = TP / (TP + FN)
(FN = False Negative)

ğŸ” Example:
Out of 10 sick patients, model caught 8 â†’ Recall = 8/10 = 0.8

ğŸ§© Syntax:
from sklearn.metrics import recall_score
recall_score(y_true, y_pred)

âš™ How It Works:
- Measures how many true positives your model successfully finds.
- High recall means fewer missed positive cases.

-------------------------------------------------------------
ğŸ“ F1 SCORE
-------------------------------------------------------------
ğŸ“– Definition:
A balance between Precision and Recall â€” good when classes are imbalanced.

ğŸ§  Formula:
F1 = 2 * (Precision * Recall) / (Precision + Recall)

ğŸ” Example:
If Precision = 0.8 and Recall = 0.6 â†’ F1 = 2 * (0.8*0.6)/(0.8+0.6) = 0.69

ğŸ§© Syntax:
from sklearn.metrics import f1_score
f1_score(y_true, y_pred)

âš™ How It Works:
- Combines precision and recall.
- High F1 means model is good at both detecting positives and avoiding false alarms.

-------------------------------------------------------------
ğŸ“ CONFUSION MATRIX
-------------------------------------------------------------
ğŸ“– Definition:
A table showing counts of predictions vs actuals for all classes.

ğŸ§© Syntax:
from sklearn.metrics import confusion_matrix
confusion_matrix(y_true, y_pred)

ğŸ” Example Output:
[[50 10]
 [5  35]]
â†’ 50 = True Negative, 10 = False Positive, 5 = False Negative, 35 = True Positive

âš™ How It Works:
- Rows = Actual values
- Columns = Predicted values
- Helps visualize model performance in detail.

=========================
âš™ï¸ REGRESSION METRICS
=========================

ğŸ’¡ Used when model predicts continuous numbers (e.g., prices, scores, temperature).

-------------------------------------------------------------
ğŸ“ MAE (Mean Absolute Error)
-------------------------------------------------------------
ğŸ“– Definition:
Average of absolute (non-negative) differences between predicted and actual values.

ğŸ§  Formula:
MAE = (Î£|y_true - y_pred|) / n

ğŸ” Example:
Actual: [10, 20, 30], Predicted: [12, 18, 33]
â†’ |10-12| + |20-18| + |30-33| = 2+2+3 = 7 â†’ 7/3 = 2.33

ğŸ§© Syntax:
from sklearn.metrics import mean_absolute_error

âš™ How It Works:
1. Take the difference between prediction and actual.
2. Remove the negative sign (absolute value).
3. Add all differences and divide by total samples.
â†’ Shows average â€œoff byâ€ amount.

-------------------------------------------------------------
ğŸ“ MSE (Mean Squared Error)
-------------------------------------------------------------
ğŸ“– Definition:
Average of squared differences between predicted and actual values.

ğŸ§  Formula:
MSE = (Î£(y_true - y_pred)Â²) / n

ğŸ” Example:
Actual: [10, 20], Predicted: [12, 17]
â†’ (10-12)Â² + (20-17)Â² = 4 + 9 = 13 â†’ 13/2 = 6.5

ğŸ§© Syntax:
from sklearn.metrics import mean_squared_error

âš™ How It Works:
1. Take each error, square it.
2. Add them all up.
3. Divide by total samples.
â†’ Large errors become more noticeable because of squaring.

-------------------------------------------------------------
ğŸ“ RMSE (Root Mean Squared Error)
-------------------------------------------------------------
ğŸ“– Definition:
Square root of MSE â€” gives a more interpretable â€œreal-worldâ€ average error.

ğŸ§  Formula:
RMSE = âˆšMSE

ğŸ” Example:
If MSE = 6.5 â†’ RMSE = âˆš6.5 = 2.55

ğŸ§© Syntax:
import numpy as np
rmse = np.sqrt(mse)

âš™ How It Works:
- Same as MSE, but takes square root.
- Makes error units same as original data (e.g., dollars, marks, temperature).

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ§­ SUMMARY TABLE

| Metric | Type | Measures | Good When |
|--------|------|-----------|------------|
| Accuracy | Classification | Overall correctness | Balanced data |
| Precision | Classification | Correct positives | False alarms costly |
| Recall | Classification | Found positives | Missing positives costly |
| F1 Score | Classification | Balance of precision & recall | Imbalanced data |
| MAE | Regression | Avg. absolute error | Simple interpretation |
| MSE | Regression | Avg. squared error | Punish large errors |
| RMSE | Regression | Root of MSE | Real-world scale error |

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ’¡ FINAL CONCEPT MEMORY TRICK:
- **Accuracy** â†’ â€œHow often am I right?â€  
- **Precision** â†’ â€œWhen I say YES, am I correct?â€  
- **Recall** â†’ â€œDid I catch all the YES cases?â€  
- **F1** â†’ â€œSmart balance of precision & recall.â€  
- **MAE** â†’ â€œOn average, how far off?â€  
- **MSE** â†’ â€œBig mistakes hit harder.â€  
- **RMSE** â†’ â€œRealistic error you can understand.â€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

